{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Seawater Data Processing\n",
    "This notebook processes the temperature, salinity, dissolved oxygen and pH data downloaded from various online sources. \n",
    "The netCDF file format is commonly used for storing multidimensional scientific data (eg. temperature, salinity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Temperature**\n",
    "\n",
    "> EN4 is the UK Met Office Hadley Centre’s subsurface temperature and salinity dataset for the global oceans, spanning 1900 to present at a monthly timestep.\n",
    "\n",
    "Each file is for a single month, therefore each file has only one time value\n",
    "\n",
    "**Fields:**\n",
    "- temperature \n",
    "- salinity\n",
    "\n",
    "https://www.metoffice.gov.uk/hadobs/en4/download-en4-2-2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "def extract_netcdf_en4(filename, date, param_OI):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        filename: str\n",
    "        date: date to extract data for\n",
    "        param_OI: parameter of interest (eg. temperature)\n",
    "    \"\"\"\n",
    "    \n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "    ds = xr.open_dataset(file_path, decode_times=False)\n",
    "    data = ds.sel(depth=ds.depth[0], time=ds.time[date])  # pressure in decibars (2.5 is the closest depth to sea-level)\n",
    "\n",
    "    # extract lat, lon and desired parameter data\n",
    "    lat = data.lat\n",
    "    lon = data.lon\n",
    "\n",
    "    if param_OI=='temperature':\n",
    "        param = data[param_OI] - 273.15\n",
    "    else:\n",
    "        param = data[param_OI]\n",
    "\n",
    "    time = float(ds.time[date])\n",
    "    date_str = convert_days_since(time, 1800, 1, 1)\n",
    "\n",
    "    return lat, lon, param, date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_temp(lat, lon, temp):\n",
    "    lats = np.arange(-90, 90, 1)\n",
    "    lons = np.arange(0, 361, 1)\n",
    "    lon_shifted = lon - 180\n",
    "    lon_wrapped = (lon_shifted + 180) % 360 - 180\n",
    "\n",
    "    df_temp = pd.DataFrame(index=lats, columns=lon_wrapped)\n",
    "    df_temp.loc[lat, lon_wrapped] = temp.values\n",
    "    df_temp = df_temp.apply(pd.to_numeric)\n",
    "    df_temp.columns = np.arange(len(df_temp.columns))\n",
    "    df_temp = df_temp[np.concatenate((np.arange(180, 360), np.arange(0, 181)))]\n",
    "    df_temp.columns = np.arange(-180,181,1)\n",
    "\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_en4_data(filename, date, param_OI):\n",
    "\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "    ds = xr.open_dataset(file_path, decode_times=False)\n",
    "\n",
    "    # Convert time values from days since 1800-01-01 to normal date format\n",
    "    time_values_days = ds['time'].values\n",
    "    start_date = pd.Timestamp('1800-01-01')\n",
    "    time_values_normal = [start_date + pd.Timedelta(days=float(days)) for days in time_values_days]\n",
    "\n",
    "    date_values = [str(dt.date()) for dt in time_values_normal]\n",
    "\n",
    "    # boolean mask for the desired date and filter\n",
    "    mask_desired_date = [d.startswith(date) for d in date_values]\n",
    "    data = ds.sel(depth=ds.depth[0], time=ds['time'][mask_desired_date])\n",
    "\n",
    "    # extract lat, lon and desired parameter data\n",
    "    lat = data.lat\n",
    "    lon = data.lon\n",
    "\n",
    "    if param_OI=='temperature':\n",
    "        param = data[param_OI] - 273.15\n",
    "    else:\n",
    "        param = data[param_OI]\n",
    "\n",
    "    return lat, lon, param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Salinity**\n",
    "This also uses the EN4 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sal(lat, lon, sal):\n",
    "    lats = np.arange(-90, 90, 1)\n",
    "    lons = np.arange(-180, 181, 1)\n",
    "\n",
    "    lon = np.where(lon > 180, lon - 360, lon)\n",
    "\n",
    "    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "    df_sal = pd.DataFrame(index=lats, columns=lons)\n",
    "    df_sal.loc[lat, lon] = sal.values\n",
    "    df_sal = df_sal.apply(pd.to_numeric)\n",
    "\n",
    "    return df_sal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dissolved Oxygen**\n",
    "> GOBAI-O2 is a gridded dataset on dissolved oxygen ranging from January 2004 to December 2022 at monthly resolution\n",
    "\n",
    "time units: days since 2004-01-01\n",
    "\n",
    "**Fields:**\n",
    "- dissolved oxygen\n",
    "\n",
    "\n",
    "https://catalog.data.gov/dataset/gobai-o2-a-global-gridded-monthly-dataset-of-ocean-interior-dissolved-oxygen-concentrations-bas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_GOBAI_O2_Data(filename, date):\n",
    "    \"\"\"\n",
    "    extracts dissolved oxygen data from the GOBAI-O2 database\n",
    "    ---\n",
    "    Inputs:\n",
    "        filename: name of the netcdf file\n",
    "        month_idx: index of the month of interest within the dataset\n",
    "\n",
    "    NB:\n",
    "        the time field has units: days since 2004-01-01\n",
    "    \"\"\"\n",
    "\n",
    "    file_path = os.path.join(base_dir, filename)\n",
    "    ds = xr.open_dataset(file_path, decode_times=False)\n",
    "\n",
    "    # Convert time values from days since 2004-01-01 to normal date format\n",
    "    time_values_days = ds['time'].values\n",
    "    start_date = pd.Timestamp('2004-01-01')\n",
    "    time_values_normal = [start_date + pd.Timedelta(days=float(days)) for days in time_values_days]\n",
    "\n",
    "    date_values = [str(dt.date()) for dt in time_values_normal]\n",
    "\n",
    "    # boolean mask for the desired date and filter\n",
    "    mask_desired_date = [d.startswith(date) for d in date_values]\n",
    "    data = ds.sel(time=ds['time'][mask_desired_date], pres=2.5) # pressure in decibars (2.5 is the closest depth to sea-level)\n",
    "\n",
    "    # extract parameter value arrays\n",
    "    lat = data.lat.values\n",
    "    lon = data.lon.values\n",
    "    doxy = data[\"oxy\"].values\n",
    "\n",
    "    # sort latitude and longitude arrays for plotting\n",
    "    lat_sorted_idx = np.argsort(lat)\n",
    "    lon_sorted_idx = np.argsort(lon)\n",
    "    lat_sorted = lat[lat_sorted_idx]\n",
    "    lon_sorted = lon[lon_sorted_idx]\n",
    "    lon_sorted_idx_2d, lat_sorted_idx_2d = np.meshgrid(lon_sorted_idx, lat_sorted_idx)\n",
    "    doxy_sorted = doxy[0, lat_sorted_idx_2d, lon_sorted_idx_2d]\n",
    "\n",
    "    return lat_sorted, lon_sorted, doxy_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_doxy(lat, lon, doxy):\n",
    "    whole_lats = np.arange(-90, 90)\n",
    "    whole_lons = np.arange(-180, 181)\n",
    "\n",
    "    lon_1d, lat_1d = np.meshgrid(lon, lat)\n",
    "    lon_flat = lon_1d.flatten()\n",
    "    lat_flat = lat_1d.flatten()\n",
    "\n",
    "    whole_lon_grid, whole_lat_grid = np.meshgrid(whole_lons, whole_lats)\n",
    "    doxy_whole_grid = griddata((lon_flat, lat_flat), doxy.flatten(), (whole_lon_grid, whole_lat_grid), method='linear')\n",
    "\n",
    "    df_doxy = pd.DataFrame(data=doxy_whole_grid, index=whole_lats, columns=whole_lons)\n",
    "\n",
    "    return df_doxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **pH**\n",
    "> OceanSODA-ETHZ is a global gridded data set of the surface ocean carbonate system for seasonal to decadal studies of ocean acidification (ETH Zürich)\n",
    "\n",
    "This dataset extrapolates in time and space the surface ocean observations of pCO2 (from the Surface Ocean CO2 ATlas (SOCAT)) and total alkalinity (TA, from the Global Ocean Data Analysis Project (GLODAP)).\n",
    "\n",
    "**Fields:**\n",
    "- pH\n",
    "\n",
    "not used:\n",
    "- salinity \n",
    "- temperature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_netcdf_dt(filename, date, param_OI):\n",
    "    \"\"\"\n",
    "    reads netcdf file and extracts data for a specific date\n",
    "    ___\n",
    "    Inputs:\n",
    "        filename: name of the netcdf file\n",
    "        date: date of interest to extract data for\n",
    "        param_OI: parameter of interest to extract data for\n",
    "            - ph_total (for pH from OceanSODA-ETHZ)\n",
    "\n",
    "    Outputs:\n",
    "        lat: 1° grid of latitude coordinates\n",
    "        lon: 1° grid of longitude coordinates\n",
    "        param: lat x lon array of parameter value for each grid cell\n",
    "    \"\"\"\n",
    "    current_dir = os.getcwd()  # Get the current working directory\n",
    "    file_path = os.path.join(current_dir, \"data\", filename)\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    data = ds.sel(time=date)\n",
    "\n",
    "    # extract lat, lon and desired parameter data\n",
    "    lat = data.lat\n",
    "    lon = data.lon\n",
    "    param = data[param_OI]\n",
    "\n",
    "    return lat, lon, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_pH(lat, lon, pH):\n",
    "\n",
    "    whole_lats = np.arange(-90, 90)\n",
    "    whole_lons = np.arange(-180, 181)\n",
    "\n",
    "    lon_1d, lat_1d = np.meshgrid(lon, lat)\n",
    "    lon_flat = lon_1d.flatten()\n",
    "    lat_flat = lat_1d.flatten()\n",
    "\n",
    "    whole_lon_grid, whole_lat_grid = np.meshgrid(whole_lons, whole_lats)\n",
    "\n",
    "    whole_lon_flat = whole_lon_grid.flatten()\n",
    "    whole_lat_flat = whole_lat_grid.flatten()\n",
    "\n",
    "    # Use the updated data to interpolate pH values onto the whole_lon_grid and whole_lat_grid\n",
    "    pH_whole_grid = griddata((lon_flat, lat_flat), pH.values.flatten(), (whole_lon_flat, whole_lat_flat), method='linear')\n",
    "\n",
    "    # Reshape the interpolated data to the original grid\n",
    "    pH_whole_grid = pH_whole_grid.reshape(whole_lon_grid.shape)\n",
    "\n",
    "    df_ph = pd.DataFrame(data=pH_whole_grid, index=whole_lats, columns=whole_lons)\n",
    "    \n",
    "    return df_ph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells combine all the en4 zip files for a specific year (eg: EN.4.2.2.analyses.g10.2000.zip, EN.4.2.2.analyses.g10.2001.zip, etc) into a single netcdf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "destination_folder = os.path.join(current_dir, \"data\")\n",
    "\n",
    "def download_files(urls, destination_folder):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    # Download files that end with '2000' up to '2023'\n",
    "    for url in urls:\n",
    "        filename = os.path.basename(url)\n",
    "        if filename.endswith(\".zip\") and filename.split(\".\")[-2].endswith(tuple(str(year) for year in range(2000, 2024))):\n",
    "            file_path = os.path.join(destination_folder, filename)\n",
    "\n",
    "            print(f\"Downloading {filename}...\")\n",
    "\n",
    "            # Download the file\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                with open(file_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"{filename} downloaded successfully.\")\n",
    "            else:\n",
    "                print(f\"Failed to download {filename}. Status code: {response.status_code}\")\n",
    "\n",
    "    print(\"All files downloaded.\")\n",
    "\n",
    "download_files(urls, destination_folder)\n",
    "\n",
    "\n",
    "import zipfile\n",
    "extraction_folder = os.path.join(destination_folder, \"en4\")\n",
    "\n",
    "def process_extracted_files(extraction_folder):\n",
    "    for root, _, files in os.walk(extraction_folder):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            print(f\"Processing {file_path}\")\n",
    "\n",
    "# Extract files from the downloaded zip files\n",
    "for root, _, files in os.walk(destination_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".zip\"):\n",
    "            zip_file_path = os.path.join(destination_folder, file)\n",
    "            with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(extraction_folder)\n",
    "\n",
    "process_extracted_files(extraction_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "def merge_nc_files(input_dir, output_file):\n",
    "    # Get a list of all NetCDF files in the input directory\n",
    "    nc_files = [f for f in os.listdir(input_dir) if f.endswith(\".nc\")]\n",
    "\n",
    "    if not nc_files:\n",
    "        print(\"No NetCDF files found in the directory.\")\n",
    "        return\n",
    "\n",
    "    data_arrays = []\n",
    "\n",
    "    for nc_file in nc_files:\n",
    "        file_path = os.path.join(input_dir, nc_file)\n",
    "        data = xr.open_dataset(file_path)\n",
    "        data_arrays.append(data)\n",
    "\n",
    "    merged_data = xr.concat(data_arrays, dim=\"time\")\n",
    "\n",
    "    merged_data.to_netcdf(output_file)\n",
    "\n",
    "    print(f\"Merged {len(nc_files)} NetCDF files into {output_file}.\")\n",
    "\n",
    "\n",
    "output_file = os.path.join(current_dir, \"data\", \"en4_2000_2023.nc\")\n",
    "merge_nc_files(extraction_folder, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dfs(date):\n",
    "    # en4_2000_2023 is a netcdf file which combines all data from 2000-2023 in a single file (above)\n",
    "    lat, lon, temp = extract_en4_data(\"datasets\\en4_2000_2023.nc\", date, \"temperature\")\n",
    "    df_temp = format_temp(lat, lon, temp)\n",
    "\n",
    "    lat, lon, sal = extract_en4_data(\"datasets\\en4_2000_2023.nc\", date, \"salinity\")\n",
    "    df_sal = format_temp(lat, lon, sal)\n",
    "\n",
    "    lat, lon, doxy = extract_GOBAI_O2_Data('datasets\\GOBAI-O2-v2.1.nc', date)\n",
    "    df_doxy = format_doxy(lat, lon, doxy)\n",
    "\n",
    "    lat, lon, pH = extract_netcdf_dt(filename='datasets\\OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc', date=date+'-15', param_OI='ph_total')\n",
    "    df_pH = format_pH(lat, lon, pH)\n",
    "\n",
    "    return df_temp, df_sal, df_doxy, df_pH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "save_directory = os.path.join(current_dir, \"data\", \"monthly_data\")\n",
    "\n",
    "for year in range(2004, 2022):\n",
    "    for i in range(1, 13):\n",
    "        month = f'{i:02}' \n",
    "        df_temp, df_sal, df_doxy, df_pH = generate_dfs(f\"{year}-{month}\")\n",
    "\n",
    "        dataframes = {\n",
    "            'df_temp': df_temp,\n",
    "            'df_sal': df_sal,\n",
    "            'df_doxy': df_doxy,\n",
    "            'df_pH': df_pH\n",
    "        }\n",
    "\n",
    "        filename = os.path.join(save_directory, f\"{year}-{month}.pickle\")\n",
    "        pickle.dump(dataframes, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_dataframes = []\n",
    "\n",
    "# Loop through each pickle file in the directory\n",
    "for filename in os.listdir(save_directory):\n",
    "    if filename.endswith(\".pickle\"):\n",
    "        filepath = os.path.join(save_directory, filename)\n",
    "        loaded_dataframes = pickle.load(open(filepath, \"rb\"))\n",
    "\n",
    "        nested_dataframes.append({\n",
    "            'df_temp': loaded_dataframes['df_temp'],\n",
    "            'df_sal': loaded_dataframes['df_sal'],\n",
    "            'df_doxy': loaded_dataframes['df_doxy'],\n",
    "            'df_pH': loaded_dataframes['df_pH']\n",
    "        })\n",
    "\n",
    "df_nested = pd.DataFrame(nested_dataframes)\n",
    "\n",
    "file_names = [os.path.splitext(file)[0] for file in os.listdir(save_directory) if file.endswith(\".pickle\")]\n",
    "df_nested.index = file_names\n",
    "\n",
    "pickle_filename = os.path.join(save_directory, \"ocean_params.pickle\")\n",
    "with open(pickle_filename, \"wb\") as file:\n",
    "    pickle.dump(df_nested, file)\n",
    "\n",
    "with open(pickle_filename, \"rb\") as file:\n",
    "    df = pickle.load(file)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
