{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Climate Models\n",
    "\n",
    "The Coupled Model Intercomparison Project (CMIP) 6 is a publicly available project under the World Climate Research Programme (WCRP) providing simulations from 120 global climate models and 45 institutions. For this project, the Norwegian Earth System Model (NorESM2) is used due its to its coverage of the four aforementioned ocean parameters influencing corrosion. \n",
    "\n",
    "The Representative Concentration Pathway (RCP), developed by the Intergovernmental Panel on Climate Change (IPCC), consists of a set of scenarios predicting the concentrations of greenhouse gases in the atmosphere until 2100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "def extract_projection(filename, date, param_OI):\n",
    "    current_dir = os.getcwd()  # Get the current working directory\n",
    "    file_path = os.path.join(current_dir, f'data\\{filename}')\n",
    "    ds = xr.open_dataset(file_path, decode_times=False)\n",
    "\n",
    "    time_values_days = ds['time'].values\n",
    "    start_date = pd.Timestamp('1800-01-01')\n",
    "    time_values_normal = [start_date + pd.Timedelta(days=float(days)) for days in time_values_days]\n",
    "    date_values = [str(dt.date()) for dt in time_values_normal]\n",
    "    mask_desired_date = [d.startswith(date) for d in date_values]\n",
    "\n",
    "\n",
    "    if param_OI == 'pH':\n",
    "        data = ds.sel(time=ds['time'][mask_desired_date], lev=0)\n",
    "        lat = data.latitude.values\n",
    "        lon = data.longitude.values\n",
    "        param = data['ph'].values\n",
    "\n",
    "    else:\n",
    "        data = ds.sel(time=ds['time'][mask_desired_date])\n",
    "        lat = data.latitude.values\n",
    "        lon = data.longitude.values\n",
    "\n",
    "        if param_OI == 'temp':\n",
    "            param = data['tos'].values\n",
    "        elif param_OI == 'sal':\n",
    "            param = data['sos'].values\n",
    "        elif param_OI == 'doxy':\n",
    "            param = data['o2satos'].values\n",
    "\n",
    "\n",
    "    return lat, lon, param[0, :, :]\n",
    "\n",
    "\n",
    "\n",
    "def generate_dfs_proj(filename, date):\n",
    "    # print(filename)\n",
    "    if filename.startswith('tos'):\n",
    "        lat, lon, param_data = extract_projection(filename, date=date, param_OI='temp')\n",
    "        param = 'temp'\n",
    "    elif filename.startswith('sos'):\n",
    "        lat, lon, param_data = extract_projection(filename, date=date, param_OI='sal')\n",
    "        param = 'sal'\n",
    "    elif filename.startswith('o2'):\n",
    "        lat, lon, param_data = extract_projection(filename, date=date, param_OI='doxy')\n",
    "        param = 'doxy'\n",
    "    elif filename.startswith('ph'):\n",
    "        lat, lon, param_data = extract_projection(filename, date=date, param_OI='pH')\n",
    "        param = 'pH'\n",
    "\n",
    "    return lat, lon, param_data, param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def days_since_to_date(days_since, ref_date=\"1800-01-01\"):\n",
    "    reference_date = datetime.strptime(ref_date, \"%Y-%m-%d\")\n",
    "    target_date = reference_date + timedelta(days=days_since)\n",
    "    \n",
    "    year = target_date.year\n",
    "    month = target_date.month\n",
    "    date_str = f'{year}-{month:02}'\n",
    "    \n",
    "    return date_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "filenames = os.listdir(current_dir)\n",
    "filenames.sort()\n",
    "\n",
    "# Group names by their starting two characters\n",
    "grouped_filenames = {}\n",
    "for filename in filenames:\n",
    "    prefix = filename[:2]\n",
    "    if prefix in grouped_filenames:\n",
    "        grouped_filenames[prefix].append(filename)\n",
    "    else:\n",
    "        grouped_filenames[prefix] = [filename]\n",
    "\n",
    "# Loop through groups \n",
    "\n",
    "group_data = {}\n",
    "\n",
    "for prefix, group in grouped_filenames.items():\n",
    "    print(f\"Group: {prefix}\")\n",
    "\n",
    "    group_dict = {} \n",
    "\n",
    "    for filename in group:\n",
    "        file_path = os.path.join(current_dir, f'data\\{filename}')\n",
    "        ds = xr.open_dataset(file_path, decode_times=False)\n",
    "\n",
    "        # Convert time values in ds to a list of dates\n",
    "        time_values = ds[\"time\"].values\n",
    "        dates = [days_since_to_date(days_since) for days_since in time_values]\n",
    "\n",
    "        for i in range(len(dates)):\n",
    "            if prefix == 'ph':\n",
    "                data = ds.sel(time=ds['time'][i], lev=0)\n",
    "                lat = data.latitude.values\n",
    "                lon = data.longitude.values\n",
    "                param = data['ph'].values\n",
    "\n",
    "            else:\n",
    "                data = ds.sel(time=ds['time'][i])\n",
    "                lat = data.latitude.values\n",
    "                lon = data.longitude.values\n",
    "\n",
    "                if prefix == 'to':\n",
    "                    param = data['tos'].values\n",
    "                elif prefix == 'so':\n",
    "                    param = data['sos'].values\n",
    "                elif prefix == 'o2':\n",
    "                    param = data['o2satos'].values\n",
    "\n",
    "            if dates[i] in group_dict:\n",
    "                group_dict[dates[i]] = pd.DataFrame(param)  \n",
    "            else:\n",
    "                group_dict[dates[i]] = pd.DataFrame(param)  \n",
    "\n",
    "            if i == 0:\n",
    "                df_lat = pd.DataFrame(lat)\n",
    "                df_lon = pd.DataFrame(lon)\n",
    "\n",
    "        group_data[prefix] = group_dict\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "all_dates = set(date for data_dict in group_data.values() for date in data_dict.keys())\n",
    "\n",
    "for date in all_dates:\n",
    "    row = {'Date': date}\n",
    "\n",
    "    for prefix, data_dict in group_data.items():\n",
    "        if date in data_dict:\n",
    "            row[prefix] = data_dict[date]\n",
    "        else:\n",
    "            row[prefix] = []\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "entire_df = pd.DataFrame(rows)\n",
    "entire_df.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the future projection data in the form of a dataframe with the parameter value, another dataframe with the latitudes and a third dataframe with the longitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle_filename = os.path.join(current_dir, \"data\", \"ocean_params_proj.pickle\")\n",
    "with open(pickle_filename, \"wb\") as file:\n",
    "    pickle.dump(entire_df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_filename = os.path.join(current_dir, \"data\", \"ocean_lat_proj.pickle\")\n",
    "with open(pickle_filename, \"wb\") as file:\n",
    "    pickle.dump(df_lat, file)\n",
    "\n",
    "pickle_filename = os.path.join(current_dir, \"data\", \"ocean_lon_proj.pickle\")\n",
    "with open(pickle_filename, \"wb\") as file:\n",
    "    pickle.dump(df_lon, file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
